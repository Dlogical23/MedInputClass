{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDpOqJE6yTJRF/Ajvak3Ug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dlogical23/MedInputClass/blob/main/MedInputClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZx8RfAJWgBL",
        "outputId": "8b2a137e-cbd1-4ae8-f5e4-2d137f635f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your CSV file is located at '/content/drive/My Drive/your_file.csv'\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "file_path = '/content/drive/MyDrive/overview-of-recordings.csv'\n",
        "try:\n",
        "  df = pd.read_csv(file_path)\n",
        "  print(\"Successfully imported CSV file.\")\n",
        "  # Now you can work with the DataFrame 'df'\n",
        "  print(df.head()) # Display first few rows of the DataFrame\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File not found at {file_path}. Please check the file path.\")\n",
        "except pd.errors.ParserError:\n",
        "  print(f\"Error: Unable to parse the CSV file at {file_path}. Please check the file format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K29xxix6YmU4",
        "outputId": "f4b33b4c-956b-42b3-9ad5-d30ebc99cb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully imported CSV file.\n",
            "   audio_clipping  audio_clipping:confidence background_noise_audible  \\\n",
            "0     no_clipping                     1.0000              light_noise   \n",
            "1  light_clipping                     0.6803                 no_noise   \n",
            "2     no_clipping                     1.0000                 no_noise   \n",
            "3     no_clipping                     1.0000              light_noise   \n",
            "4     no_clipping                     1.0000                 no_noise   \n",
            "\n",
            "   background_noise_audible:confidence  overall_quality_of_the_audio  \\\n",
            "0                               1.0000                          3.33   \n",
            "1                               0.6803                          3.33   \n",
            "2                               0.6655                          3.33   \n",
            "3                               1.0000                          3.33   \n",
            "4                               1.0000                          4.67   \n",
            "\n",
            "     quiet_speaker  quiet_speaker:confidence  speaker_id  \\\n",
            "0  audible_speaker                       1.0    43453425   \n",
            "1  audible_speaker                       1.0    43719934   \n",
            "2  audible_speaker                       1.0    43719934   \n",
            "3  audible_speaker                       1.0    31349958   \n",
            "4  audible_speaker                       1.0    43719934   \n",
            "\n",
            "                                       file_download  \\\n",
            "0  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n",
            "1  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n",
            "2  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n",
            "3  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n",
            "4  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n",
            "\n",
            "                       file_name  \\\n",
            "0  1249120_43453425_58166571.wav   \n",
            "1  1249120_43719934_43347848.wav   \n",
            "2  1249120_43719934_53187202.wav   \n",
            "3  1249120_31349958_55816195.wav   \n",
            "4  1249120_43719934_82524191.wav   \n",
            "\n",
            "                                              phrase            prompt  \\\n",
            "0                    When I remember her I feel down    Emotional pain   \n",
            "1  When I carry heavy things I feel like breaking...  Hair falling out   \n",
            "2          there is too much pain when i move my arm       Heart hurts   \n",
            "3  My son had his lip pierced and it is swollen a...    Infected wound   \n",
            "4             My muscles in my lower back are aching    Infected wound   \n",
            "\n",
            "   writer_id  \n",
            "0   21665495  \n",
            "1   44088126  \n",
            "2   44292353  \n",
            "3   43755034  \n",
            "4   21665495  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "# Extract texts (medical transcriptions)\n",
        "texts = df[\"phrase\"].tolist()\n",
        "\n",
        "# Extract labels (diagnosis codes or symptoms)\n",
        "labels = df[\"prompt\"].tolist()\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=25)  # Adjust num_labels\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "class MedicalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MedicalDataset(train_encodings, train_labels)\n",
        "test_dataset = MedicalDataset(test_encodings, test_labels)\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "# Load dataset (replace with your medical transcription dataset)\n",
        "texts = [...]  # List of medical transcriptions\n",
        "labels = [...]  # Corresponding labels (e.g., diagnosis codes)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=25)  # Adjust num_labels\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "class MedicalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MedicalDataset(train_encodings, train_labels)\n",
        "test_dataset = MedicalDataset(test_encodings, test_labels)\n",
        "\n",
        "# Training setup (continued)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    for batch in test_loader:\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            total_eval_accuracy += accuracy_score(labels.cpu(), predictions.cpu())\n",
        "\n",
        "    avg_eval_accuracy = total_eval_accuracy / len(test_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Accuracy: {avg_eval_accuracy}\")\n",
        "\n",
        "# Save the model, tokenizer, and label mappings\n",
        "model.save_pretrained(\"BertModelAndTokenizer\")\n",
        "tokenizer.save_pretrained(\"BertModelAndTokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wlkm9A2Ygn-",
        "outputId": "a9fe79b5-c069-45d2-d27b-5b6ef83f5153"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.5872604846954346\n",
            "Epoch 1, Accuracy: 0.0\n",
            "Epoch 2, Loss: 3.3486216068267822\n",
            "Epoch 2, Accuracy: 0.0\n",
            "Epoch 3, Loss: 3.1659557819366455\n",
            "Epoch 3, Accuracy: 0.0\n",
            "Epoch 4, Loss: 3.1196954250335693\n",
            "Epoch 4, Accuracy: 0.0\n",
            "Epoch 5, Loss: 3.1146955490112305\n",
            "Epoch 5, Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('BertModelAndTokenizer/tokenizer_config.json',\n",
              " 'BertModelAndTokenizer/special_tokens_map.json',\n",
              " 'BertModelAndTokenizer/vocab.txt',\n",
              " 'BertModelAndTokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"BertModelAndTokenizer\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"BertModelAndTokenizer\")\n",
        "\n",
        "# Move the model to the appropriate device (CPU or GPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlNXmZIlf1KS",
        "outputId": "5bab948a-3297-4d39-aeb8-4e22d91a42a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=25, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example new medical transcription data\n",
        "new_texts = [\n",
        "    \"Patient presents with chest pain and shortness of breath.\",\n",
        "    \"History of diabetes and hypertension.\"\n",
        "]\n",
        "\n",
        "# Tokenize the new data\n",
        "new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Move the tokenized data to the same device as the model\n",
        "input_ids = new_encodings['input_ids'].to(device)\n",
        "attention_mask = new_encodings['attention_mask'].to(device)"
      ],
      "metadata": {
        "id": "J0n2tAJ0f_mS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Convert predictions to labels (if you have label mappings)\n",
        "# Update label_mappings to include all possible labels (0-24)\n",
        "label_mappings = {i: f\"Condition {i}\" for i in range(25)}\n",
        "predicted_labels = [label_mappings[pred.item()] for pred in predictions]\n",
        "\n",
        "# Print the results\n",
        "for text, label in zip(new_texts, predicted_labels):\n",
        "    print(f\"Text: {text}\\nPredicted Label: {label}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNGMQ2PogGqU",
        "outputId": "eec78d3b-ccc9-450a-e835-54c8f4814504"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Patient presents with chest pain and shortness of breath.\n",
            "Predicted Label: Condition 18\n",
            "\n",
            "Text: History of diabetes and hypertension.\n",
            "Predicted Label: Condition 18\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPLOYMENT"
      ],
      "metadata": {
        "id": "JqYx4o_hgq_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDKXSyvqgqfa",
        "outputId": "f2a401ef-87dd-415b-a040-f2a06e1a7e18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.8 starlette-0.45.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"BertModelAndTokenizer\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"BertModelAndTokenizer\")\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    texts = data.get(\"texts\", [])\n",
        "\n",
        "    # Tokenize the input texts\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "from fastapi import FastAPI, Request\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"BertModelAndTokenizer\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"BertModelAndTokenizer\")\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    texts = data.get(\"texts\", [])\n",
        "\n",
        "    # Tokenize the input texts\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Convert predictions to labels\n",
        "    label_mappings = {0: \"Condition A\", 1: \"Condition B\", 2: \"Condition C\"}  # Replace with your actual label mappings\n",
        "    predicted_labels = [label_mappings.get(pred.item(), \"Unknown\") for pred in predictions]\n",
        "\n",
        "    return {\"predictions\": predicted_labels}\n",
        "\n",
        "# Run the API\n",
        "# Use `uvicorn <filename>:app --reload` to start the server\n",
        "    return {\"predictions\": predicted_labels}\n",
        "\n",
        "# Run the API\n",
        "# Use `uvicorn <filename>:app --reload` to start the server"
      ],
      "metadata": {
        "id": "PvbQ6DBWhOAv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE-TUNING"
      ],
      "metadata": {
        "id": "cWE-rI7EhkwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame from the CSV file\n",
        "texts = df[\"phrase\"].tolist()\n",
        "labels = df[\"prompt\"].tolist()\n",
        "\n",
        "# Create a mapping from labels to numerical IDs\n",
        "unique_labels = list(set(labels))\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert labels to numerical IDs using the mapping\n",
        "labels = [label_to_id[label] for label in labels]\n",
        "\n",
        "# Now perform the train_test_split\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    texts, labels, test_size=0.3, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "EV5kf_kUr4nt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)  # Adjust test_size as needed\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42) # Splitting temp into val and tesT\n",
        "\n",
        "# Tokenize data for all three sets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)  # Tokenize validation data\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Create Datasets for all three sets\n",
        "train_dataset = MedicalDataset(train_encodings, train_labels)\n",
        "val_dataset = MedicalDataset(val_encodings, val_labels)  # Create validation dataset\n",
        "test_dataset = MedicalDataset(test_encodings, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 3  # Adjust based on performance\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    for batch in val_loader:\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            total_eval_accuracy += accuracy_score(labels.cpu(), predictions.cpu())\n",
        "\n",
        "    avg_eval_accuracy = total_eval_accuracy / len(val_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Validation Accuracy: {avg_eval_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2X4ZqtLhn2d",
        "outputId": "267850c0-f192-40fd-ac9a-2c3a6092c95f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.688893748064564\n",
            "Epoch 1, Validation Accuracy: 0.8042800453514739\n",
            "Epoch 2, Loss: 1.194597749269172\n",
            "Epoch 2, Validation Accuracy: 0.9573412698412699\n",
            "Epoch 3, Loss: 0.465682319429231\n",
            "Epoch 3, Validation Accuracy: 0.9831349206349206\n"
          ]
        }
      ]
    }
  ]
}